{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom fastprogress import progress_bar\nimport seaborn as sns\nfrom matplotlib.colors import LinearSegmentedColormap\nfrom scipy.stats import skew\nfrom scipy.signal import find_peaks\nimport matplotlib.patches as mpatches\nfrom collections import Counter\nimport os\nimport torch\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nimport torch.nn as nn\nimport torch.optim as optim\nimport time\nfrom numpy.random import choice\nimport numba\nimport random \nimport ctypes\nimport multiprocessing as mp\n!pip install wandb\nimport wandb\nimport math\nimport torch.nn.functional as F\nimport sklearn","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-30T13:08:59.058617Z","iopub.execute_input":"2022-06-30T13:08:59.059638Z","iopub.status.idle":"2022-06-30T13:09:08.094368Z","shell.execute_reply.started":"2022-06-30T13:08:59.059602Z","shell.execute_reply":"2022-06-30T13:09:08.092826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nclass encoder():\n\n  def __init__(self):\n    self.encode_dict={}\n    self.last_value=None\n    self.last_table= pd.DataFrame() #cache for fast dataloading (if we decide to use it as a model attribute)\n    self.use_cache_flag=False\n    self.codes_per_column=[]\n\n  def use_cache(self,flag=True):\n    self.use_cache_flag=True \n\n  def get_encode_dict(self):\n    return self.encode_dict\n  \n  def set_encode_dict(self,encode_dict):\n    self.encode_dict=encode_dict \n\n  def encode_column(self,table,column_name):\n    #column is an array of strings to encode\n    #make sure to create dict before\n    tmp=[self.encode_dict[str(table[column_name][i])+column_name] for i in range(len(table))]\n    \n    return tmp\n\n  def create_dict(self,table):\n    \n    self.codes_per_column=[]\n    for column in table.columns:\n      tmp=[]\n      if isinstance(table[column][0],str):\n        tmp=table[column].unique()\n        \n        for idx,name in enumerate(tmp):\n            self.encode_dict[str(name)+column]=idx \n      self.codes_per_column.append(len(tmp))\n\n\n  def encode_table(self,table,update_cache=False): \n    \n    if self.use_cache_flag: #using cached value\n      return self.last_table\n\n    str_columns = [] #create encoding dictionnary if there isn't one\n    if self.encode_dict=={}:\n  \n      self.create_dict(table)\n      \n    out=table.copy() \n    for column in table.columns:\n      num=0\n      if isinstance(table[column][0],str): #encode string columns\n        out[column]=self.encode_column(table,column) \n    \n    #updating cache\n    if self.last_table.empty or update_cache: \n      self.last_table=out\n        \n    return torch.tensor(self.codes_per_column),out\n\n\n\n\n\ndef age_enc(table):\n  tmp=[]\n  for group in table:\n    if group[-2:].isdigit():\n      tmp.append(int(group[-2:]))\n    else:\n      tmp.append(int(group[:2]))\n  return tmp\n\n\n\n# frequency encoding\ndef encode_FE(df1, df2, cols):\n    for col in cols:\n        df = pd.concat([df1[col],df2[col]])\n        vc = df.value_counts(dropna=True, normalize=True).to_dict()\n        vc[-1] = -1\n        nm = col+'_FE'\n        \n        df1[nm] = df1[col].map(vc)\n        df1[nm] = df1[nm].astype('float32')\n        df2[nm] = df2[col].map(vc)\n        df2[nm] = df2[nm].astype('float32')\n        print(nm,', ',end='')\n\n\ndef encode_AG(main_columns, uids,train_df,test_df,aggregations=['mean'],fillna=True, usena=False):\n    # AGGREGATION OF MAIN WITH UID FOR GIVEN STATISTICS\n    for main_column in main_columns:  \n        for col in uids:\n            for agg_type in aggregations:\n                new_col_name = main_column+'_'+col+'_'+agg_type\n                temp_df = pd.concat([train_df[[col, main_column]], test_df[[col,main_column]]])\n                if usena: temp_df.loc[temp_df[main_column]==-1,main_column] = np.nan\n                temp_df = temp_df.groupby([col])[main_column].agg([agg_type]).reset_index().rename(\n                                                        columns={agg_type: new_col_name})\n\n                temp_df.index = list(temp_df[col])\n                temp_df = temp_df[new_col_name].to_dict()   \n\n                train_df[new_col_name] = train_df[col].map(temp_df).astype('float32')\n                test_df[new_col_name]  = test_df[col].map(temp_df).astype('float32')\n                \n                if fillna:\n                    train_df[new_col_name].fillna(-1,inplace=True)\n                    test_df[new_col_name].fillna(-1,inplace=True)\n                \n                print(\"'\"+new_col_name+\"'\",', ',end='')\n\ndef combine(col1,col2, train_df, test_df, fillna=True):\n  \n  dfs=[train_df,test_df]\n  new_name=col1+'_'+col2\n  train_df[new_name]=train_df[col1].fillna(-1).astype('str')+'_'+train_df[col2].fillna(-1).astype('str')\n  test_df[new_name]=test_df[col1].fillna(-1).astype('str')+'_'+test_df[col2].fillna(-1).astype('str')\n  \n  return(new_name)\n\ndef update_weights(accuracy,allocated):\n  accuracy=1/(accuracy+0.00001)\n  accuracy=np.exp(10*accuracy/accuracy.sum())\n  accuracy=allocated*accuracy/accuracy.sum()\n\n  return torch.tensor(accuracy).float()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:09:58.098375Z","iopub.execute_input":"2022-06-30T13:09:58.098733Z","iopub.status.idle":"2022-06-30T13:09:58.126062Z","shell.execute_reply.started":"2022-06-30T13:09:58.098703Z","shell.execute_reply":"2022-06-30T13:09:58.124811Z"},"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_path='../input/ai4d-dataset'\n#@title Import tables and set-up the encoder\n#encoding tables\ntrain_table=pd.read_csv(os.path.join(input_path,\"Train.csv\"))\ntrain_orig=train_table.fillna(-1)\ntrain_table=train_orig\n\ntest_table=pd.read_csv(os.path.join(input_path,\"Test.csv\"))\n\n\nencoder_=encoder() #encoder\n\ntest_orig=test_table.fillna(-1)\ntest_table=test_orig\n","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:20:44.51732Z","iopub.execute_input":"2022-06-30T13:20:44.517675Z","iopub.status.idle":"2022-06-30T13:20:44.648133Z","shell.execute_reply.started":"2022-06-30T13:20:44.517644Z","shell.execute_reply":"2022-06-30T13:20:44.646871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Feature engineering - selection ","metadata":{}},{"cell_type":"code","source":"# 'country', 'age_group', 'travel_with', 'total_female',\n#        'total_male', 'purpose', 'main_activity', 'info_source',\n#        'tour_arrangement','first_trip_tz'\n\n#feature engineering \ntar=train_table.pop('cost_category')\n\n\ntmp=age_enc(train_table.age_group)\ntrain_table[\"has_more_money\"]=[1 if tmp[i]>18 else 0 for i in range(len(tmp))]\n\ntmp=age_enc(test_table.age_group)\ntest_table[\"has_more_money\"]=[1 if tmp[i]>18 else 0 for i in range(len(tmp))]\n\n\ntrain_table[\"length\"]=train_table['night_mainland']+train_table['night_zanzibar']\ntrain_table[\"total_num\"]=train_table['total_female']+train_table['total_male']\ntrain_table[\"ratio_male\"]=train_table['total_male']/train_table['total_num']\n\ntest_table[\"length\"]=test_table['night_mainland']+test_table['night_zanzibar']\ntest_table[\"total_num\"]=test_table['total_female']+test_table['total_male']\ntest_table[\"fem2male\"]=test_table['total_female']-test_table['total_male']\n# test_table[\"ratio_male\"]=test_table['total_male']/test_table['total_num']\nfe_combo_names,ag_combo_names=[],[]\n\n\n# # combo_names.append(combine('travel_with','ratio', train_df=train_table,test_df=test_table))\nag_combo_names.append(combine('tour_arrangement','length', train_df=train_table,test_df=test_table))\nag_combo_names.append(combine('tour_arrangement','total_num', train_df=train_table,test_df=test_table))\n# ag_combo_names.append(combine('tour_arrangement','first_trip_tz', train_df=train_table,test_df=test_table))\nag_combo_names.append(combine('purpose','main_activity', train_df=train_table,test_df=test_table))\n# # combo_names.append(combine('country','total_num', train_df=train_table,test_df=test_table))\n# # combo_names.append(combine('info_source','tour_arrangement', train_df=train_table,test_df=test_table))\n# fe_combo_names.append(combine('purpose','main_activity', train_df=train_table,test_df=test_table))\n#fe_combo_names.append(combine('package_guided_tour','main_activity', train_df=train_table,test_df=test_table))\n# fe_combo_names.append(combine('package_sightseeing','main_activity', train_df=train_table,test_df=test_table))\nfe_combo_names.append(combine('package_accomodation','total_num', train_df=train_table,test_df=test_table))\n# fe_combo_names.append(combine('package_guided_tour','total_num', train_df=train_table,test_df=test_table))\nag_combo_names.append(combine('package_accomodation','length', train_df=train_table,test_df=test_table))\n# # combo_names.append(combine('package_accomodation','total_num', train_df=train_table,test_df=test_table))\n# # combo_names.append(combine('package_guided_tour','length', train_df=train_table,test_df=test_table))\n# # combo_names.append(combine('package_guided_tour','total_num', train_df=train_table,test_df=test_table))\nname1=combine('package_accomodation','package_food', train_df=train_table,test_df=test_table)\nname2=combine('package_transport_tz','package_transport_int', train_df=train_table,test_df=test_table)\nname3=combine(name1,name2, train_df=train_table,test_df=test_table)\nfeatures_to_drop=[name1,name2,'purpose','main_activity','info_source']\n\n# ag_combo_names.append(combine('total_num','age_group', train_df=train_table,test_df=test_table))\n# # combo_names.append(combine('length','total_num', train_df=train_table,test_df=test_table))\nag_combo_names.append(combine('age_group','travel_with', train_df=train_table,test_df=test_table))\n\n\n# train_table.drop(['night_mainland','night_zanzibar','total_female','total_male','purpose','main_activity'], inplace=True, axis=1)\n# test_table.drop(['night_mainland','night_zanzibar','total_female','total_male','purpose','main_activity'], inplace=True, axis=1)\n\n\n# # combo_names.append(combine('main_activity','tour_arrangement',train_df=train_table,test_df=test_table))\n# #concatenate the two table to get encoding dict\n\ntrain_table['cost_category']=tar\ntest_tmp=pd.concat([train_table.iloc[:,:-1],test_table])\ntest_tmp[train_table.columns[-1]]=pd.concat([train_table.iloc[:,-1],train_table.iloc[:len(test_table),-1]])\ntest_tmp=test_tmp.reset_index(drop=True)\n\nnum_codes,_=encoder_.encode_table(test_tmp.iloc[:,1:]) # encode table\n\ntrain_table=encoder_.encode_table(train_table.iloc[:,1:].fillna(-1))[1] # encode table\ntest_table=encoder_.encode_table(test_table.iloc[:,1:].fillna(-1))[1]\n\n\n\n\n\nencode_FE(train_table,test_table,[name1,name2,'first_trip_tz'])\n\n# train_table.drop([name1,name2], inplace=True, axis=1)\n# test_table.drop([name1,name2], inplace=True, axis=1)\n\ntar=train_table.pop('cost_category')\nencode_FE(train_table,test_table,['travel_with','purpose'])\n# encode_AG([name1,name2],['country'],train_df=train_table,test_df=test_table,aggregations=['mean'])\nencode_AG([ 'package_accomodation','package_food','purpose_main_activity'],['country'],train_df=train_table,test_df=test_table,aggregations=['mean'])\n# encode_AG(['total_num','length'],['package_accomodation','package_food','package_transport_tz','package_transport_int'],train_df=train_table,test_df=test_table,aggregations=['mean'])\n# encode_AG(['tour_arrangement'],['total_male','total_female'],train_df=train_table,test_df=test_table,aggregations=['mean'])\n\n# encode_AG(['package_transport_int', 'package_accomodation',\n#       'package_food', 'package_transport_tz', 'package_sightseeing',\n#       'package_guided_tour', 'package_insurance','first_trip_tz'], combo_names[:6],train_df=train_table,test_df=test_table, aggregations=['mean'])\n\n#encode_AG(combo_names[6:], combo_names[:6],train_df=train_table,test_df=test_table, aggregations=['mean'])\n# encode_AG(['package_transport_int', 'package_accomodation','package_food', 'package_transport_tz', 'package_sightseeing',\n#        'package_guided_tour', 'package_insurance', 'first_trip_tz'],ag_combo_names+['length','total_num','country'],train_df=train_table,test_df=test_table, aggregations=['mean','std'])\n\n\ntrain_table.drop(features_to_drop, inplace=True, axis=1)\ntest_table.drop(features_to_drop, inplace=True, axis=1)\n\ntrain_table['cost_category']=tar\n\n\n\n\n\n##########################################################################\n\ntrain_data=torch.tensor(train_table.to_numpy())\ntest_data=torch.tensor(test_table.to_numpy())\ntest_data=test_data[:,:]\n\nratio=0.2\nidx=torch.randperm(train_data.shape[0])\ntrain_targets,val_targets=train_data[idx[:-int(ratio*len(train_data))],-1],train_data[idx[-int(ratio*len(train_data)):],-1]\nval_data=train_data[idx[-int(ratio*len(train_data)):],:-1]\ntrain_data=train_data[idx[:-int(ratio*len(train_data))],:-1]\n\ncolumn_names=test_table.columns\nprint(column_names)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:20:57.876879Z","iopub.execute_input":"2022-06-30T13:20:57.877302Z","iopub.status.idle":"2022-06-30T13:21:07.994694Z","shell.execute_reply.started":"2022-06-30T13:20:57.87727Z","shell.execute_reply":"2022-06-30T13:21:07.993518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from xgboost import XGBRegressor\n\n# tmp_train=combiner_model.eval()(combiner_train_input)[0]\n# tmp_val=combiner_model.eval()(combiner_val_input)[0]\n# tmp_test=combiner_model.eval()(combiner_test_input)[0]\n\n# tmp_test,tmp_train,tmp_val=tmp_test.detach().numpy(),tmp_train.detach().numpy(),tmp_val.detach().numpy()\n\nstate=np.random.randint(0,10000)\nmodel = XGBRegressor(n_estimators=3000,\n            max_depth=12,\n            learning_rate=0.02,\n            subsample=0.8,\n            colsample_bytree=0.4,\n            missing=-1,\n            random_state=912,#5106\n            eval_metric=[\"mlogloss\"],\n            objective='multi:softprob',\n            num_class=6,\n            tree_method='gpu_hist')\n\nmodel.fit(train_data.numpy(),train_targets.numpy(),verbose=25,eval_set=[(val_data.numpy(),val_targets.numpy())],early_stopping_rounds=200)\npredicted = model.predict(val_data.numpy())\nprint(predicted)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:23:29.992222Z","iopub.execute_input":"2022-06-30T13:23:29.992814Z","iopub.status.idle":"2022-06-30T13:23:30.038038Z","shell.execute_reply.started":"2022-06-30T13:23:29.992779Z","shell.execute_reply":"2022-06-30T13:23:30.036822Z"},"trusted":true},"execution_count":null,"outputs":[]}]}